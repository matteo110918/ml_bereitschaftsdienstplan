{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Data Preparation",
   "id": "70682b6a9a6861f2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T15:10:57.416591Z",
     "start_time": "2024-10-31T15:10:56.834857Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import holidays\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import SelectFromModel\n"
   ],
   "id": "e1eef165a7028e8",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T15:10:57.452119Z",
     "start_time": "2024-10-31T15:10:57.422622Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Laden der Daten\n",
    "# Passe den Pfad entsprechend an, falls die Daten in einem spezifischen Ordner liegen\n",
    "df = pd.read_csv('../data/raw/sickness_table.csv', parse_dates=['date'])"
   ],
   "id": "776cc4fb00d1a906",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1. Datenbereinigung",
   "id": "68c8344d506ab936"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In der Datenanalyse wurden folgende Aspekte der Datenqualität überprüft:\n",
    "\n",
    "- **Fehlende Werte:** Es wurde festgestellt, dass keine fehlenden Werte in den Daten vorliegen. Somit sind keine Imputationsmethoden oder Auffüllstrategien notwendig.\n",
    "- **Duplikate:** Die Daten wurden auf Duplikate überprüft, und es konnte bestätigt werden, dass keine doppelten Einträge vorhanden sind. Ein Entfernen doppelter Zeilen ist daher nicht erforderlich.\n",
    "- **Ausreißer:** Während der Analyse wurden mögliche Ausreißer in den numerischen Variablen untersucht. Die vorhandenen Ausreißer scheinen jedoch alle plausibel und nachvollziehbar zu sein (z. B. Spitzenwerte bei Notrufen oder Krankenständen zu bestimmten Zeiten). Daher wurde entschieden, die Ausreißer nicht weiter zu beschneiden oder zu entfernen."
   ],
   "id": "800f6e0b7abd60b6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Forcast der zukünftigen Werte",
   "id": "7bd714046241c545"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T15:23:43.782290Z",
     "start_time": "2024-10-31T15:23:37.066409Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "import pandas as pd\n",
    "\n",
    "def forecast_n_sick_calls(df, forecast_days=30):\n",
    "    # Zeilenweise Prognosen für `n_sick` und `calls` erstellen und zurückgeben\n",
    "    forecasts = {}\n",
    "    \n",
    "    for feature in ['n_sick', 'calls',  'n_duty']:\n",
    "        series = df[feature]\n",
    "        \n",
    "        # ARIMA-Modell (anpassbar für SARIMA, Prophet, etc.)\n",
    "        model = ARIMA(series, order=(5,1,2))  # Beispielhafte ARIMA-Parameter\n",
    "        model_fit = model.fit()\n",
    "        \n",
    "        # Prognose erstellen\n",
    "        forecast = model_fit.forecast(steps=forecast_days)\n",
    "        forecasts[feature] = forecast.values  # Werte als Array speichern\n",
    "    \n",
    "    # DataFrame für die prognostizierten Werte\n",
    "    forecast_df = pd.DataFrame(forecasts)\n",
    "    forecast_df['date'] = pd.date_range(start=df['date'].max() + pd.Timedelta(days=1), periods=forecast_days, freq='D')\n",
    "    \n",
    "    return forecast_df\n",
    "\n",
    "# Beispiel für die Anwendung der Prognosefunktion\n",
    "forecast_df = forecast_n_sick_calls(df)\n",
    "forecast_df\n"
   ],
   "id": "ef644fc31c2fd513",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\palmierima\\PycharmProjects\\modelEngineeringCaseStudy2\\.venv\\Lib\\site-packages\\statsmodels\\tsa\\statespace\\sarimax.py:966: UserWarning: Non-stationary starting autoregressive parameters found. Using zeros as starting parameters.\n",
      "  warn('Non-stationary starting autoregressive parameters'\n",
      "C:\\Users\\palmierima\\PycharmProjects\\modelEngineeringCaseStudy2\\.venv\\Lib\\site-packages\\statsmodels\\tsa\\statespace\\sarimax.py:978: UserWarning: Non-invertible starting MA parameters found. Using zeros as starting parameters.\n",
      "  warn('Non-invertible starting MA parameters found.'\n",
      "C:\\Users\\palmierima\\PycharmProjects\\modelEngineeringCaseStudy2\\.venv\\Lib\\site-packages\\statsmodels\\base\\model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\palmierima\\PycharmProjects\\modelEngineeringCaseStudy2\\.venv\\Lib\\site-packages\\statsmodels\\tsa\\statespace\\sarimax.py:978: UserWarning: Non-invertible starting MA parameters found. Using zeros as starting parameters.\n",
      "  warn('Non-invertible starting MA parameters found.'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "       n_sick        calls  n_duty       date\n",
       "0   75.045053  9369.061436  1900.0 2019-05-28\n",
       "1   74.606011  9377.074530  1900.0 2019-05-29\n",
       "2   78.512621  9419.346746  1900.0 2019-05-30\n",
       "3   77.095856  9431.380945  1900.0 2019-05-31\n",
       "4   74.605025  9450.345845  1900.0 2019-06-01\n",
       "5   76.817678  9426.644482  1900.0 2019-06-02\n",
       "6   78.331633  9449.798352  1900.0 2019-06-03\n",
       "7   75.380769  9425.835888  1900.0 2019-06-04\n",
       "8   75.225242  9448.411572  1900.0 2019-06-05\n",
       "9   78.230000  9426.058741  1900.0 2019-06-06\n",
       "10  77.060186  9447.990866  1900.0 2019-06-07\n",
       "11  74.579670  9426.392287  1900.0 2019-06-08\n",
       "12  76.845080  9447.677062  1900.0 2019-06-09\n",
       "13  78.311283  9426.725317  1900.0 2019-06-10\n",
       "14  75.405659  9447.358155  1900.0 2019-06-11\n",
       "15  75.232772  9427.044133  1900.0 2019-06-12\n",
       "16  78.203706  9447.045476  1900.0 2019-06-13\n",
       "17  77.059145  9427.351972  1900.0 2019-06-14\n",
       "18  74.607116  9446.742265  1900.0 2019-06-15\n",
       "19  76.833909  9427.650394  1900.0 2019-06-16\n",
       "20  78.289465  9446.448380  1900.0 2019-06-17\n",
       "21  75.426290  9427.939739  1900.0 2019-06-18\n",
       "22  75.245358  9446.163485  1900.0 2019-06-19\n",
       "23  78.177520  9428.220249  1900.0 2019-06-20\n",
       "24  77.058241  9445.887293  1900.0 2019-06-21\n",
       "25  74.633600  9428.492191  1900.0 2019-06-22\n",
       "26  76.823078  9445.619538  1900.0 2019-06-23\n",
       "27  78.267880  9428.755824  1900.0 2019-06-24\n",
       "28  75.446645  9445.359962  1900.0 2019-06-25\n",
       "29  75.257836  9429.011404  1900.0 2019-06-26"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_sick</th>\n",
       "      <th>calls</th>\n",
       "      <th>n_duty</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>75.045053</td>\n",
       "      <td>9369.061436</td>\n",
       "      <td>1900.0</td>\n",
       "      <td>2019-05-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>74.606011</td>\n",
       "      <td>9377.074530</td>\n",
       "      <td>1900.0</td>\n",
       "      <td>2019-05-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>78.512621</td>\n",
       "      <td>9419.346746</td>\n",
       "      <td>1900.0</td>\n",
       "      <td>2019-05-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>77.095856</td>\n",
       "      <td>9431.380945</td>\n",
       "      <td>1900.0</td>\n",
       "      <td>2019-05-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>74.605025</td>\n",
       "      <td>9450.345845</td>\n",
       "      <td>1900.0</td>\n",
       "      <td>2019-06-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>76.817678</td>\n",
       "      <td>9426.644482</td>\n",
       "      <td>1900.0</td>\n",
       "      <td>2019-06-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>78.331633</td>\n",
       "      <td>9449.798352</td>\n",
       "      <td>1900.0</td>\n",
       "      <td>2019-06-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>75.380769</td>\n",
       "      <td>9425.835888</td>\n",
       "      <td>1900.0</td>\n",
       "      <td>2019-06-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>75.225242</td>\n",
       "      <td>9448.411572</td>\n",
       "      <td>1900.0</td>\n",
       "      <td>2019-06-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>78.230000</td>\n",
       "      <td>9426.058741</td>\n",
       "      <td>1900.0</td>\n",
       "      <td>2019-06-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>77.060186</td>\n",
       "      <td>9447.990866</td>\n",
       "      <td>1900.0</td>\n",
       "      <td>2019-06-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>74.579670</td>\n",
       "      <td>9426.392287</td>\n",
       "      <td>1900.0</td>\n",
       "      <td>2019-06-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>76.845080</td>\n",
       "      <td>9447.677062</td>\n",
       "      <td>1900.0</td>\n",
       "      <td>2019-06-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>78.311283</td>\n",
       "      <td>9426.725317</td>\n",
       "      <td>1900.0</td>\n",
       "      <td>2019-06-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>75.405659</td>\n",
       "      <td>9447.358155</td>\n",
       "      <td>1900.0</td>\n",
       "      <td>2019-06-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>75.232772</td>\n",
       "      <td>9427.044133</td>\n",
       "      <td>1900.0</td>\n",
       "      <td>2019-06-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>78.203706</td>\n",
       "      <td>9447.045476</td>\n",
       "      <td>1900.0</td>\n",
       "      <td>2019-06-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>77.059145</td>\n",
       "      <td>9427.351972</td>\n",
       "      <td>1900.0</td>\n",
       "      <td>2019-06-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>74.607116</td>\n",
       "      <td>9446.742265</td>\n",
       "      <td>1900.0</td>\n",
       "      <td>2019-06-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>76.833909</td>\n",
       "      <td>9427.650394</td>\n",
       "      <td>1900.0</td>\n",
       "      <td>2019-06-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>78.289465</td>\n",
       "      <td>9446.448380</td>\n",
       "      <td>1900.0</td>\n",
       "      <td>2019-06-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>75.426290</td>\n",
       "      <td>9427.939739</td>\n",
       "      <td>1900.0</td>\n",
       "      <td>2019-06-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>75.245358</td>\n",
       "      <td>9446.163485</td>\n",
       "      <td>1900.0</td>\n",
       "      <td>2019-06-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>78.177520</td>\n",
       "      <td>9428.220249</td>\n",
       "      <td>1900.0</td>\n",
       "      <td>2019-06-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>77.058241</td>\n",
       "      <td>9445.887293</td>\n",
       "      <td>1900.0</td>\n",
       "      <td>2019-06-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>74.633600</td>\n",
       "      <td>9428.492191</td>\n",
       "      <td>1900.0</td>\n",
       "      <td>2019-06-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>76.823078</td>\n",
       "      <td>9445.619538</td>\n",
       "      <td>1900.0</td>\n",
       "      <td>2019-06-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>78.267880</td>\n",
       "      <td>9428.755824</td>\n",
       "      <td>1900.0</td>\n",
       "      <td>2019-06-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>75.446645</td>\n",
       "      <td>9445.359962</td>\n",
       "      <td>1900.0</td>\n",
       "      <td>2019-06-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>75.257836</td>\n",
       "      <td>9429.011404</td>\n",
       "      <td>1900.0</td>\n",
       "      <td>2019-06-26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2. Feature Engineering",
   "id": "253f58111de5616"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2.1. Feature Enrichment",
   "id": "359091737e48cf60"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\palmierima\\AppData\\Local\\Temp\\2\\ipykernel_25196\\162832584.py:35: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  self.month_avg = X.groupby('month')[self.feature_columns].mean()\n",
      "C:\\Users\\palmierima\\AppData\\Local\\Temp\\2\\ipykernel_25196\\162832584.py:36: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  self.combined_avg = X.groupby(['day_of_week', 'week', 'month'])[self.feature_columns].mean()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "   sby_need month day_of_week week quarter season  holiday  weekend  \\\n",
       "0       4.0     4           4   13       2      2    False    False   \n",
       "1      70.0     4           5   13       2      2    False     True   \n",
       "2       0.0     4           6   13       2      2    False     True   \n",
       "3       0.0     4           0   14       2      2    False    False   \n",
       "4       0.0     4           1   14       2      2    False    False   \n",
       "\n",
       "   n_sick_avg_month  calls_avg_month  n_duty_avg_month  n_sick_avg_combined  \\\n",
       "0            63.675           8266.4            1825.0                73.00   \n",
       "1            63.675           8266.4            1825.0                54.50   \n",
       "2            63.675           8266.4            1825.0                59.00   \n",
       "3            63.675           8266.4            1825.0                65.75   \n",
       "4            63.675           8266.4            1825.0                65.25   \n",
       "\n",
       "   calls_avg_combined  n_duty_avg_combined  \n",
       "0              8154.0               1700.0  \n",
       "1              9150.0               1750.0  \n",
       "2              8080.0               1800.0  \n",
       "3              9676.5               1825.0  \n",
       "4              9132.0               1825.0  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sby_need</th>\n",
       "      <th>month</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>week</th>\n",
       "      <th>quarter</th>\n",
       "      <th>season</th>\n",
       "      <th>holiday</th>\n",
       "      <th>weekend</th>\n",
       "      <th>n_sick_avg_month</th>\n",
       "      <th>calls_avg_month</th>\n",
       "      <th>n_duty_avg_month</th>\n",
       "      <th>n_sick_avg_combined</th>\n",
       "      <th>calls_avg_combined</th>\n",
       "      <th>n_duty_avg_combined</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>63.675</td>\n",
       "      <td>8266.4</td>\n",
       "      <td>1825.0</td>\n",
       "      <td>73.00</td>\n",
       "      <td>8154.0</td>\n",
       "      <td>1700.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>70.0</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>63.675</td>\n",
       "      <td>8266.4</td>\n",
       "      <td>1825.0</td>\n",
       "      <td>54.50</td>\n",
       "      <td>9150.0</td>\n",
       "      <td>1750.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>63.675</td>\n",
       "      <td>8266.4</td>\n",
       "      <td>1825.0</td>\n",
       "      <td>59.00</td>\n",
       "      <td>8080.0</td>\n",
       "      <td>1800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>63.675</td>\n",
       "      <td>8266.4</td>\n",
       "      <td>1825.0</td>\n",
       "      <td>65.75</td>\n",
       "      <td>9676.5</td>\n",
       "      <td>1825.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>63.675</td>\n",
       "      <td>8266.4</td>\n",
       "      <td>1825.0</td>\n",
       "      <td>65.25</td>\n",
       "      <td>9132.0</td>\n",
       "      <td>1825.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3,
   "source": [
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "import holidays\n",
    "\n",
    "# Custom Transformer für kalendarische Features und Feiertage\n",
    "class DateFeatureExtractor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, date_column='date'):\n",
    "        self.date_column = date_column\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        years = range(X[self.date_column].dt.year.min(), X[self.date_column].dt.year.max() + 1)\n",
    "        self.de_holidays = holidays.Germany(years=years)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        X['month'] = X[self.date_column].dt.month.astype('category')\n",
    "        X['day_of_week'] = X[self.date_column].dt.weekday.astype('category')\n",
    "        X['week'] = X[self.date_column].dt.isocalendar().week.astype('category')\n",
    "        X['quarter'] = X[self.date_column].dt.quarter.astype('category')\n",
    "        X['season'] = (X['month'].astype(int) % 12 // 3 + 1).astype('category')  # 1=Winter, 2=Spring, etc.\n",
    "        X['holiday'] = X[self.date_column].apply(lambda x: 1 if x in self.de_holidays else 0).astype(bool)\n",
    "        X['weekend'] = X['day_of_week'].apply(lambda x: 1 if x >= 5 else 0).astype(bool)\n",
    "        return X\n",
    "\n",
    "# Custom Transformer für saisonale Durchschnittswerte\n",
    "class SeasonalAverages(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, feature_columns, groupby_columns):\n",
    "        self.feature_columns = feature_columns\n",
    "        self.groupby_columns = groupby_columns\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        # Durchschnittswerte für die Ebenen Monat und Kombination aus Wochentag, Woche und Monat\n",
    "        self.month_avg = X.groupby('month')[self.feature_columns].mean()\n",
    "        self.combined_avg = X.groupby(['day_of_week', 'week', 'month'])[self.feature_columns].mean()\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        \n",
    "        # Hinzufügen der monatlichen Durchschnittswerte\n",
    "        for feature in self.feature_columns:\n",
    "            X[f'{feature}_avg_month'] = X['month'].map(self.month_avg[feature]).astype(float)\n",
    "        \n",
    "        # Erstellen eines Index für die kombinierte Granularität\n",
    "        X['time_index'] = X[self.groupby_columns].astype(str).agg('_'.join, axis=1)\n",
    "        combined_avg = self.combined_avg.reset_index()\n",
    "        combined_avg['time_index'] = combined_avg[self.groupby_columns].astype(str).agg('_'.join, axis=1)\n",
    "        \n",
    "        # Hinzufügen der Durchschnittswerte für die kombinierte Granularität\n",
    "        for feature in self.feature_columns:\n",
    "            avg_col_name_combined = f'{feature}_avg_combined'\n",
    "            X = X.merge(combined_avg[['time_index', feature]].rename(columns={feature: avg_col_name_combined}),\n",
    "                        on='time_index', how='left')\n",
    "        \n",
    "        # Entfernen des temporären Indexes\n",
    "        X.drop(columns=['time_index'], inplace=True)\n",
    "        return X\n",
    "\n",
    "# Custom Transformer zum Entfernen unerwünschter Spalten\n",
    "class DropColumns(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns_to_drop):\n",
    "        self.columns_to_drop = columns_to_drop\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return X.drop(columns=self.columns_to_drop)\n",
    "\n",
    "# Feature-Engineering-Pipeline\n",
    "feature_engineering_pipeline = Pipeline([\n",
    "    ('date_features', DateFeatureExtractor(date_column='date')),\n",
    "    ('seasonal_averages', SeasonalAverages(\n",
    "        feature_columns=['n_sick', 'calls', 'n_duty'],\n",
    "        groupby_columns=['day_of_week', 'week', 'month']\n",
    "    )),\n",
    "    ('drop_unnecessary_columns', DropColumns(columns_to_drop=['Unnamed: 0', 'n_sby', 'dafted', 'n_sick', 'calls', 'date', 'n_duty']))\n",
    "])\n",
    "\n",
    "# Pipeline auf die Daten anwenden\n",
    "df_transformed = feature_engineering_pipeline.fit_transform(df)\n",
    "\n",
    "# Ergebnisse anzeigen\n",
    "df_transformed.head()\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "a4e6526a47e27701"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Train-Test-Split",
   "id": "5d1fcc228c3514c9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 4,
   "source": [
    "# Setze die Zielvariable\n",
    "target = 'sby_need'\n",
    "\n",
    "# Definiere die Merkmale (X) und die Zielvariable (y)\n",
    "X = df_transformed.drop(columns=[target])\n",
    "y = df_transformed[target]\n",
    "\n",
    "# Führe einen zeitlichen Train-Test-Split durch (80 % Training, 20 % Test) mit shuffle=False\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n"
   ],
   "id": "5f96d16dd7a34f35"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2.2. Scaling und Encoding",
   "id": "d00ac13acf0c1762"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 5,
   "source": [
    "# Pipeline für Lineare Regression und SVR (mit Scaling, Encoding und PCA)\n",
    "preprocessor_lr_svr = Pipeline(steps=[\n",
    "    ('preprocessor', ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', StandardScaler(), make_column_selector(dtype_include=['int64', 'float64'])),\n",
    "            ('cat', OneHotEncoder(drop='first', sparse_output=False), make_column_selector(dtype_include='category'))\n",
    "        ], remainder='passthrough')),# Behalte alle übrigen Spalten im Originalzustand\n",
    "    ('pca', PCA(n_components=0.95)) # Behalte 95 % der Varianz\n",
    "])\n",
    "\n",
    "# Pipeline für Tree-basierte Modelle (nur Encoding und Feature Selection, ohne Skalierung)\n",
    "preprocessor_tree = Pipeline(steps=[\n",
    "    ('preprocessor', ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', 'passthrough', make_column_selector(dtype_include=['int64', 'float64'])),\n",
    "            ('cat', OneHotEncoder(drop='first', sparse_output=False), make_column_selector(dtype_include='category'))\n",
    "        ], remainder='passthrough')),  # Behalte alle übrigen Spalten im Originalzustand\n",
    "    ('feature_selection', SelectFromModel(RandomForestRegressor(n_estimators=100, random_state=42), threshold='0.001*mean', max_features=20))\n",
    "])\n"
   ],
   "id": "4d77f42918e4500d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model Training",
   "id": "aefac3cc9887a02e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 6,
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score"
   ],
   "id": "17772a1b4ad35b0e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.36750000e+01, 8.26640000e+03, 1.82500000e+03, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [6.36750000e+01, 8.26640000e+03, 1.82500000e+03, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.00000000e+00],\n",
       "       [6.36750000e+01, 8.26640000e+03, 1.82500000e+03, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.00000000e+00],\n",
       "       ...,\n",
       "       [8.52473118e+01, 7.75541935e+03, 1.80000000e+03, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.00000000e+00],\n",
       "       [8.52473118e+01, 7.75541935e+03, 1.80000000e+03, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.00000000e+00],\n",
       "       [8.52473118e+01, 7.75541935e+03, 1.80000000e+03, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7,
   "source": [
    "# Daten aufteilen und sicherstellen, dass `y` nur für `fit_transform` übergeben wird\n",
    "X_train_tree_fs = preprocessor_tree.fit_transform(X_train, y_train)  # `y` hier verwenden\n",
    "X_test_tree_fs = preprocessor_tree.transform(X_test)  # Kein `y` hier verwenden\n",
    "\n",
    "X_train_lr_pca = preprocessor_lr_svr.fit_transform(X_train)  # `y` für PCA-Pipeline optional\n",
    "X_test_lr_pca = preprocessor_lr_svr.transform(X_test)  # Kein `y` hier verwenden\n",
    "\n",
    "X_train_tree_fs"
   ],
   "id": "96026f1171fdaa94"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lineare Regression - Evaluation:\n",
      "MSE: 10834.593657884678\n",
      "MAE: 60.14573595397706\n",
      "R^2: 0.04163031242014126\n",
      "\n",
      "Random Forest - Evaluation:\n",
      "MSE: 11185.13849953585\n",
      "MAE: 49.42279165495398\n",
      "R^2: 0.010623007394771733\n",
      "\n",
      "Gradient Boosting - Evaluation:\n",
      "MSE: 10678.689898296363\n",
      "MAE: 48.91398849911376\n",
      "R^2: 0.055420717680097376\n",
      "\n",
      "Support Vector Regression - Evaluation:\n",
      "MSE: 14047.213878222294\n",
      "MAE: 53.40774215579572\n",
      "R^2: -0.24254073580713587\n"
     ]
    }
   ],
   "execution_count": 9,
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# 1. Baseline-Modell: Lineare Regression\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train_lr_pca, y_train)\n",
    "y_pred_lr = lr_model.predict(X_test_lr_pca)\n",
    "\n",
    "# Evaluierung der Linearen Regression\n",
    "print(\"Lineare Regression - Evaluation:\")\n",
    "print(\"MSE:\", mean_squared_error(y_test, y_pred_lr))\n",
    "print(\"MAE:\", mean_absolute_error(y_test, y_pred_lr))\n",
    "print(\"R^2:\", r2_score(y_test, y_pred_lr))\n",
    "\n",
    "# 2. Random Forest\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train_tree_fs, y_train)\n",
    "y_pred_rf = rf_model.predict(X_test_tree_fs)\n",
    "\n",
    "# Evaluierung des Random Forest\n",
    "print(\"\\nRandom Forest - Evaluation:\")\n",
    "print(\"MSE:\", mean_squared_error(y_test, y_pred_rf))\n",
    "print(\"MAE:\", mean_absolute_error(y_test, y_pred_rf))\n",
    "print(\"R^2:\", r2_score(y_test, y_pred_rf))\n",
    "\n",
    "# 3. Gradient Boosting\n",
    "gb_model = GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
    "gb_model.fit(X_train_tree_fs, y_train)\n",
    "y_pred_gb = gb_model.predict(X_test_tree_fs)\n",
    "\n",
    "# Evaluierung des Gradient Boosting\n",
    "print(\"\\nGradient Boosting - Evaluation:\")\n",
    "print(\"MSE:\", mean_squared_error(y_test, y_pred_gb))\n",
    "print(\"MAE:\", mean_absolute_error(y_test, y_pred_gb))\n",
    "print(\"R^2:\", r2_score(y_test, y_pred_gb))\n",
    "\n",
    "# 4. Support Vector Regression\n",
    "svr_model = SVR(kernel='rbf', C=1.0, epsilon=0.1)  # Standardwerte, können angepasst werden\n",
    "svr_model.fit(X_train_lr_pca, y_train)  # PCA transformierte Daten verwenden\n",
    "y_pred_svr = svr_model.predict(X_test_lr_pca)\n",
    "\n",
    "# Evaluierung der Support Vector Regression\n",
    "print(\"\\nSupport Vector Regression - Evaluation:\")\n",
    "print(\"MSE:\", mean_squared_error(y_test, y_pred_svr))\n",
    "print(\"MAE:\", mean_absolute_error(y_test, y_pred_svr))\n",
    "print(\"R^2:\", r2_score(y_test, y_pred_svr))\n",
    "\n"
   ],
   "id": "2c374abff979614f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T15:11:04.442241Z",
     "start_time": "2024-10-31T15:11:04.441249Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "6ddbbaf9a5ce6a73",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
